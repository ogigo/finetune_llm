{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6871796,"sourceType":"datasetVersion","datasetId":3940771},{"sourceId":6933142,"sourceType":"datasetVersion","datasetId":3977323},{"sourceId":6935922,"sourceType":"datasetVersion","datasetId":3982911}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Installation des librairies manquantes","metadata":{}},{"cell_type":"code","source":"! pip uninstall transformers -y\n! pip install transformers\n! pip install bitsandbytes\n! pip install einops\n! pip install peft\n#! pip install trl","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:20:49.217500Z","iopub.execute_input":"2023-12-09T13:20:49.217796Z","iopub.status.idle":"2023-12-09T13:21:52.755186Z","shell.execute_reply.started":"2023-12-09T13:20:49.217768Z","shell.execute_reply":"2023-12-09T13:21:52.754049Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bug selon la version de datasets, besoin d'installer une version plus récente que celle de l'environnement pré-installé :\n! pip uninstall datasets -y\n! pip install datasets==2.13.1","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:21:52.757057Z","iopub.execute_input":"2023-12-09T13:21:52.757361Z","iopub.status.idle":"2023-12-09T13:22:07.871567Z","shell.execute_reply.started":"2023-12-09T13:21:52.757335Z","shell.execute_reply":"2023-12-09T13:22:07.870642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\nimport datasets\nprint(datasets.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:07.872921Z","iopub.execute_input":"2023-12-09T13:22:07.873226Z","iopub.status.idle":"2023-12-09T13:22:13.874827Z","shell.execute_reply.started":"2023-12-09T13:22:07.873199Z","shell.execute_reply":"2023-12-09T13:22:13.873934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Restaurant dataset","metadata":{}},{"cell_type":"code","source":"import einops\nimport torch\nfrom datasets import load_dataset,Dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\nfrom peft import LoraConfig,get_peft_model\n#from trl import SFTTrainer,DataCollatorForCompletionOnlyLM","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:13.877476Z","iopub.execute_input":"2023-12-09T13:22:13.878339Z","iopub.status.idle":"2023-12-09T13:22:23.510348Z","shell.execute_reply.started":"2023-12-09T13:22:13.878305Z","shell.execute_reply":"2023-12-09T13:22:23.509373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset0 = load_dataset(\"Argen7um/restrant-qa\")['train'].select(range(877))\ndataset0","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:23.511545Z","iopub.execute_input":"2023-12-09T13:22:23.511920Z","iopub.status.idle":"2023-12-09T13:22:25.438812Z","shell.execute_reply.started":"2023-12-09T13:22:23.511872Z","shell.execute_reply":"2023-12-09T13:22:25.437953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset0['Prompt'][0]","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:25.440028Z","iopub.execute_input":"2023-12-09T13:22:25.440312Z","iopub.status.idle":"2023-12-09T13:22:25.450145Z","shell.execute_reply.started":"2023-12-09T13:22:25.440288Z","shell.execute_reply":"2023-12-09T13:22:25.449311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"removed_text = '\\nUsers may communicate with you in English or Chinese, and you should respond in the language in which the question was asked. \\nEnsure to maintain a hospitable and supportive tone, embodying the warm and welcoming spirit of our restaurant. Your goal is to enhance the dining experience of our guests by facilitating seamless and delightful interactions, ensuring they receive accurate and helpful information to make their dining experience memorable\\n[context]:'\n#\"You possess extensive knowledge about the restaurant’s menu, operating hours, reservation policies, food ingredients, special diet accommodations, and health and safety practices.'\n#Feel free to assist the users by providing informative and friendly responses to questions about restaurant locations, directions, parking facilities, and public transport access. Additionally, you are adept at explaining menu items, offering suggestions, explaining dish ingredients, and helping with special dietary requests such as vegetarian, vegan, gluten-free, and allergy-specific needs.\ntext = []\nfor i in range(877):\n    text.append(dataset0['Prompt'][i].replace(removed_text,''))","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:25.451406Z","iopub.execute_input":"2023-12-09T13:22:25.451708Z","iopub.status.idle":"2023-12-09T13:22:27.707864Z","shell.execute_reply.started":"2023-12-09T13:22:25.451683Z","shell.execute_reply":"2023-12-09T13:22:27.706897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndataset_df = pd.DataFrame(columns = ['Prompt'])\ndataset_df['Prompt'] = text\ndataset1 = Dataset.from_pandas(dataset_df)\ndataset1","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:27.709270Z","iopub.execute_input":"2023-12-09T13:22:27.710070Z","iopub.status.idle":"2023-12-09T13:22:27.726210Z","shell.execute_reply.started":"2023-12-09T13:22:27.710031Z","shell.execute_reply":"2023-12-09T13:22:27.725298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    print('\\n')\n    print(dataset1['Prompt'][i])","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:27.727318Z","iopub.execute_input":"2023-12-09T13:22:27.727614Z","iopub.status.idle":"2023-12-09T13:22:27.752781Z","shell.execute_reply.started":"2023-12-09T13:22:27.727583Z","shell.execute_reply":"2023-12-09T13:22:27.751890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nquestion = []\nresponse = []\nfor i in range(877):\n    question.append(dataset0['train'][i]['Prompt'].split('[question]:')[1].split('[answer]:')[0].replace(' [/INST]\\n',''))\n    response.append(dataset0['train'][i]['Prompt'].split('[question]:')[1].split('[answer]:')[1].replace('</s>',''))\nimport pandas as pd\ndata = pd.DataFrame(columns = ['question','answer'])\ndata['question'] = question\ndata['answer'] = response\ndata\n\ndataset1 = Dataset.from_pandas(data)\ndataset1\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:27.756846Z","iopub.execute_input":"2023-12-09T13:22:27.757175Z","iopub.status.idle":"2023-12-09T13:22:27.765383Z","shell.execute_reply.started":"2023-12-09T13:22:27.757150Z","shell.execute_reply":"2023-12-09T13:22:27.764576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" ","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:27.766438Z","iopub.execute_input":"2023-12-09T13:22:27.766743Z","iopub.status.idle":"2023-12-09T13:22:29.200615Z","shell.execute_reply.started":"2023-12-09T13:22:27.766719Z","shell.execute_reply":"2023-12-09T13:22:29.199615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(element):\n    return tokenizer(\n        element[\"Prompt\"],\n        truncation=True,\n        max_length=2048,\n        add_special_tokens=False,\n    )\n\ndataset_tokenized = dataset1.map(\n    tokenize, \n    batched=True, \n    #num_proc=os.cpu_count(),    # multithreaded\n    remove_columns=[\"Prompt\"]     # don't need the strings anymore, we have tokens from here on\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:29.201742Z","iopub.execute_input":"2023-12-09T13:22:29.202050Z","iopub.status.idle":"2023-12-09T13:22:29.689942Z","shell.execute_reply.started":"2023-12-09T13:22:29.202023Z","shell.execute_reply":"2023-12-09T13:22:29.689052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer('[answer]'),tokenizer.decode([24115])","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:29.691132Z","iopub.execute_input":"2023-12-09T13:22:29.691530Z","iopub.status.idle":"2023-12-09T13:22:29.698073Z","shell.execute_reply.started":"2023-12-09T13:22:29.691502Z","shell.execute_reply":"2023-12-09T13:22:29.697165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate(elements):\n    tokenlist=[e[\"input_ids\"] for e in elements]\n    tokens_maxlen=max([len(t) for t in tokenlist])  # length of longest input\n    input_ids,labels,attention_masks = [],[],[]\n    \n    for tokens in tokenlist:\n        \n        # how many pad tokens to add for this sample\n        index_answer = tokens.index(24115) # on récupère l'index de début de la réponse\n        tokens_question = tokens[:index_answer] # on s'arrête à la réponse\n        pad_len_question =tokens_maxlen-len(tokens_question)\n        pad_len_answer = tokens_maxlen-len(tokens[index_answer+2:])\n        \n        input_ids.append( tokens_question + [tokenizer.pad_token_id]*pad_len_question )   \n        labels.append( tokens[index_answer+2:] + [tokenizer.pad_token_id]*pad_len_answer )    \n        attention_masks.append( [1]*len(tokens_question) + [0]*pad_len_question ) \n\n    batch={\n        \"input_ids\": torch.tensor(input_ids),\n        \"labels\": torch.tensor(labels),\n        \"attention_mask\": torch.tensor(attention_masks)\n    }\n    return batch","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:29.699078Z","iopub.execute_input":"2023-12-09T13:22:29.699334Z","iopub.status.idle":"2023-12-09T13:22:29.708127Z","shell.execute_reply.started":"2023-12-09T13:22:29.699305Z","shell.execute_reply":"2023-12-09T13:22:29.707362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndef collate(elements):\n    tokenlist=[e[\"input_ids\"] for e in elements]\n    tokens_maxlen=max([len(t) for t in tokenlist])  # length of longest input\n    input_ids,labels,attention_masks = [],[],[]\n    \n    for tokens in tokenlist:\n        \n        # how many pad tokens to add for this sample\n        index_answer = tokens.index(24115)\n        pad_len=tokens_maxlen-len(tokens)\n        pad_len_answer = tokens_maxlen-len(tokens[index_answer+2:])\n        \n        # pad input_ids with pad_token, labels with ignore_index (-100) and set attention_mask 1 where content, otherwise 0\n        input_ids.append( tokens + [tokenizer.pad_token_id]*pad_len )   \n        labels.append( tokens[index_answer+2:] + [tokenizer.pad_token_id]*pad_len_answer )    \n        attention_masks.append( [1]*len(tokens) + [0]*pad_len ) \n\n    batch={\n        \"input_ids\": torch.tensor(input_ids),\n        \"labels\": torch.tensor(labels),\n        \"attention_mask\": torch.tensor(attention_masks)\n    }\n    return batch\n    \n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:22:29.709217Z","iopub.execute_input":"2023-12-09T13:22:29.709477Z","iopub.status.idle":"2023-12-09T13:22:29.721363Z","shell.execute_reply.started":"2023-12-09T13:22:29.709454Z","shell.execute_reply":"2023-12-09T13:22:29.720573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Téléchargement du model pre-entrainé ","metadata":{"id":"rjOMoSbGSxx9"}},{"cell_type":"code","source":"# BitsAndBytes permet le fine tuning avec \"quantification\" pour réduire l'impact mémoire et les calculs\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        \"mistralai/Mistral-7B-Instruct-v0.1\",\n        #\"facebook/opt-350m\",\n        device_map=\"auto\",\n        torch_dtype=torch.float16, #torch.bfloat16,\n        trust_remote_code=True\n            )","metadata":{"id":"ZwXZbQ2dSwzI","outputId":"a57e521a-a8a3-48e9-a478-63334083f94a","execution":{"iopub.status.busy":"2023-12-09T13:22:29.722475Z","iopub.execute_input":"2023-12-09T13:22:29.722803Z","iopub.status.idle":"2023-12-09T13:24:41.409338Z","shell.execute_reply.started":"2023-12-09T13:22:29.722771Z","shell.execute_reply":"2023-12-09T13:24:41.408390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configuration du peft LoRa","metadata":{"id":"NuAx3zBeUL1q"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64 #128\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\n        \"Wqkv\",\n        \"out_proj\",\n        \"up_proj\",\n        \"down_proj\",\n    ])\n","metadata":{"id":"dQdvjTYTT1vQ","execution":{"iopub.status.busy":"2023-12-09T13:24:41.416265Z","iopub.execute_input":"2023-12-09T13:24:41.416559Z","iopub.status.idle":"2023-12-09T13:24:41.423540Z","shell.execute_reply.started":"2023-12-09T13:24:41.416534Z","shell.execute_reply":"2023-12-09T13:24:41.422660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:24:41.424824Z","iopub.execute_input":"2023-12-09T13:24:41.425193Z","iopub.status.idle":"2023-12-09T13:24:42.281084Z","shell.execute_reply.started":"2023-12-09T13:24:41.425161Z","shell.execute_reply":"2023-12-09T13:24:42.280113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_dir = \"/kaggle/working/\"\nper_device_train_batch_size = 1\ngradient_accumulation_steps = 16 \noptim = \"paged_adamw_32bit\"\nsave_steps = 55 \nlogging_steps = 55\nlearning_rate = 1e-4\nmax_grad_norm = 0.3\nmax_steps = 55 * 14 \nwarmup_ratio = 0.03\nlr_scheduler_type = \"linear\"\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    #save_steps=save_steps,\n    logging_steps=logging_steps,\n    save_strategy= 'no', #''epoch',\n    #evaluation_strategy = \"steps\",#\"epoch\",\n    learning_rate=learning_rate,\n    fp16=True,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=True,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to = 'none',\n    save_total_limit = 1\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:24:42.282365Z","iopub.execute_input":"2023-12-09T13:24:42.282675Z","iopub.status.idle":"2023-12-09T13:24:42.290980Z","shell.execute_reply.started":"2023-12-09T13:24:42.282649Z","shell.execute_reply":"2023-12-09T13:24:42.290121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    data_collator=collate,\n    train_dataset=dataset_tokenized,\n    #eval_dataset=lm_datasets, #dataset_tokenized[\"test\"],\n    args=training_arguments,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:24:42.292028Z","iopub.execute_input":"2023-12-09T13:24:42.292286Z","iopub.status.idle":"2023-12-09T13:24:42.354989Z","shell.execute_reply.started":"2023-12-09T13:24:42.292263Z","shell.execute_reply":"2023-12-09T13:24:42.354284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:24:42.355972Z","iopub.execute_input":"2023-12-09T13:24:42.356232Z","iopub.status.idle":"2023-12-09T13:24:57.894084Z","shell.execute_reply.started":"2023-12-09T13:24:42.356209Z","shell.execute_reply":"2023-12-09T13:24:57.892660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.save_model('/kaggle/working/')","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:24:57.895492Z","iopub.status.idle":"2023-12-09T13:24:57.896017Z","shell.execute_reply.started":"2023-12-09T13:24:57.895740Z","shell.execute_reply":"2023-12-09T13:24:57.895765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nfrom transformers import AutoTokenizer\nfrom peft import AutoPeftModelForCausalLM\n\nmodel = AutoPeftModelForCausalLM.from_pretrained(\"/kaggle/input/mistral\", \n                                                  load_in_4bit=True\n                                                )\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/mistral\")\n\"\"\"\n\n#text = \"\"\"Below is a question from Human. Write a response.\\n    \n### Question:\\n Is it recommended to base clinical and therapeutic decisions solely on ChatGPT's\n#knowledge.?\\n    \n### Response:\"\"\"\n\"\"\"\ninputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\nout = model.generate(**inputs, max_new_tokens=50)\n\nprint(tokenizer.decode(out[0]))\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:24:57.897347Z","iopub.status.idle":"2023-12-09T13:24:57.897817Z","shell.execute_reply.started":"2023-12-09T13:24:57.897570Z","shell.execute_reply":"2023-12-09T13:24:57.897595Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]}]}