{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# install the required libraries\n\n! pip install accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.30.2 trl==0.4.7 --quiet","metadata":{"execution":{"iopub.status.busy":"2023-12-09T18:46:42.007940Z","iopub.execute_input":"2023-12-09T18:46:42.008209Z","iopub.status.idle":"2023-12-09T18:46:59.586160Z","shell.execute_reply.started":"2023-12-09T18:46:42.008184Z","shell.execute_reply":"2023-12-09T18:46:59.585107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set up the weights and bias secret key for model performence tracking\n\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\n\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"WANDB_API_KEY\") \n\nwandb.login(key=wandb_api)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:13.162365Z","iopub.execute_input":"2023-12-09T11:58:13.162684Z","iopub.status.idle":"2023-12-09T11:58:16.065859Z","shell.execute_reply.started":"2023-12-09T11:58:13.162653Z","shell.execute_reply":"2023-12-09T11:58:16.064955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ignoring the warnings\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:16.066954Z","iopub.execute_input":"2023-12-09T11:58:16.067438Z","iopub.status.idle":"2023-12-09T11:58:16.071466Z","shell.execute_reply.started":"2023-12-09T11:58:16.067412Z","shell.execute_reply":"2023-12-09T11:58:16.070600Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset preparation","metadata":{}},{"cell_type":"code","source":"# Load the dataset from HF - https://huggingface.co/datasets/knkarthick/dialogsum\n\nfrom datasets import load_dataset\n\ndataset_name = 'knkarthick/dialogsum' \n\nds = load_dataset(dataset_name)\n\n# looking at the dataset splits\nds","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:16.073435Z","iopub.execute_input":"2023-12-09T11:58:16.073813Z","iopub.status.idle":"2023-12-09T11:58:18.708114Z","shell.execute_reply.started":"2023-12-09T11:58:16.073788Z","shell.execute_reply":"2023-12-09T11:58:18.707295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the full train dataset and subset of test dataset\n\ntrain_ds , test_ds = load_dataset(dataset_name,split =['train', 'test[0:200]'])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-09T11:58:18.709327Z","iopub.execute_input":"2023-12-09T11:58:18.709625Z","iopub.status.idle":"2023-12-09T11:58:19.039954Z","shell.execute_reply.started":"2023-12-09T11:58:18.709599Z","shell.execute_reply":"2023-12-09T11:58:19.039182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# covert the dataset to pandas dataframe for instruction finetuning dataset preparation\n\nimport pandas as pd\n\ntrain_df = pd.DataFrame(train_ds)\ntest_df = pd.DataFrame(test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:19.040911Z","iopub.execute_input":"2023-12-09T11:58:19.041147Z","iopub.status.idle":"2023-12-09T11:58:20.061065Z","shell.execute_reply.started":"2023-12-09T11:58:19.041124Z","shell.execute_reply":"2023-12-09T11:58:20.060263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at the training dataset\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:20.062629Z","iopub.execute_input":"2023-12-09T11:58:20.063014Z","iopub.status.idle":"2023-12-09T11:58:20.078232Z","shell.execute_reply.started":"2023-12-09T11:58:20.062976Z","shell.execute_reply":"2023-12-09T11:58:20.077281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Write a concise summary of the following text which starts with ### Input: \\n\nReturn your response in bullet points which covers the key points of the text.","metadata":{}},{"cell_type":"code","source":"# instruction finetuning data preparation function\n\ndef prepare_dataset(df,split='train'):\n    text_col = []\n    instruction = \"\"\"Write a concise summary of the below input text.Return your response in bullet points which covers the key points of the text. \"\"\" # change instuction according to the task\n    if split == 'train':\n        for _ , row in df.iterrows():\n            input_q = row[\"dialogue\"]\n            output = row[\"summary\"]\n            text = (\"### Instruction: \\n\" + instruction + \n                    \"\\n### Input: \\n\" + input_q + \n                    \"\\n### Response :\\n\" + output) # keeping output column in training dataset\n            text_col.append(text)\n        df.loc[:,'text'] = text_col\n    else:\n        for _ , row in df.iterrows():\n            input_q = row[\"dialogue\"]\n            text = (\"### Instruction: \\n\" + instruction + \n                    \"\\n### Input: \\n\" + input_q +\n                    \"\\n### Response :\\n\" ) # not keeping output column in test dataset\n            text_col.append(text)\n        df.loc[:,'text'] = text_col\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:20.079512Z","iopub.execute_input":"2023-12-09T11:58:20.080341Z","iopub.status.idle":"2023-12-09T11:58:20.087767Z","shell.execute_reply.started":"2023-12-09T11:58:20.080305Z","shell.execute_reply":"2023-12-09T11:58:20.086650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = prepare_dataset(train_df,'train')\ntest_df = prepare_dataset(test_df,'test')","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:20.089112Z","iopub.execute_input":"2023-12-09T11:58:20.089407Z","iopub.status.idle":"2023-12-09T11:58:20.810895Z","shell.execute_reply.started":"2023-12-09T11:58:20.089381Z","shell.execute_reply":"2023-12-09T11:58:20.810138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at the train df , new text column is created\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:20.815150Z","iopub.execute_input":"2023-12-09T11:58:20.815943Z","iopub.status.idle":"2023-12-09T11:58:20.826122Z","shell.execute_reply.started":"2023-12-09T11:58:20.815906Z","shell.execute_reply":"2023-12-09T11:58:20.825156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at one of the train text column format\nprint(train_df['text'][0])","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:20.827407Z","iopub.execute_input":"2023-12-09T11:58:20.828076Z","iopub.status.idle":"2023-12-09T11:58:20.838647Z","shell.execute_reply.started":"2023-12-09T11:58:20.828042Z","shell.execute_reply":"2023-12-09T11:58:20.837667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at the test df , new text column is created\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:20.839739Z","iopub.execute_input":"2023-12-09T11:58:20.839998Z","iopub.status.idle":"2023-12-09T11:58:20.855718Z","shell.execute_reply.started":"2023-12-09T11:58:20.839974Z","shell.execute_reply":"2023-12-09T11:58:20.854770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at one of the test text column format without output data\nprint(test_df['text'][0])","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:20.856882Z","iopub.execute_input":"2023-12-09T11:58:20.857183Z","iopub.status.idle":"2023-12-09T11:58:20.866418Z","shell.execute_reply.started":"2023-12-09T11:58:20.857158Z","shell.execute_reply":"2023-12-09T11:58:20.865409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# coverting the dataframe to huggingface dataset for easy finetuning\nfrom datasets import Dataset\ndataset = Dataset.from_pandas(train_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:20.867552Z","iopub.execute_input":"2023-12-09T11:58:20.867823Z","iopub.status.idle":"2023-12-09T11:58:20.930051Z","shell.execute_reply.started":"2023-12-09T11:58:20.867801Z","shell.execute_reply":"2023-12-09T11:58:20.929281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# looking at the dataset\ndataset","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:20.931199Z","iopub.execute_input":"2023-12-09T11:58:20.931697Z","iopub.status.idle":"2023-12-09T11:58:20.938164Z","shell.execute_reply.started":"2023-12-09T11:58:20.931661Z","shell.execute_reply":"2023-12-09T11:58:20.937284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading the sharded Llama-2 model in Quantized format","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer\n\n# sharded model path in hugging face\nmodel_name = \"TinyPixel/Llama-2-7B-bf16-sharded\"\n\n#model_name = 'NousResearch/Llama-2-7b-hf'\n\n# Quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n)\n\n# loading the model with quantization config\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    device_map='auto'\n)\nmodel.config.use_cache = False","metadata":{"execution":{"iopub.status.busy":"2023-12-09T11:58:20.939282Z","iopub.execute_input":"2023-12-09T11:58:20.939542Z","iopub.status.idle":"2023-12-09T12:00:37.800963Z","shell.execute_reply.started":"2023-12-09T11:58:20.939519Z","shell.execute_reply":"2023-12-09T12:00:37.800193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the Llama-2 tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True , return_token_type_ids=False)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-12-09T12:00:37.802067Z","iopub.execute_input":"2023-12-09T12:00:37.802745Z","iopub.status.idle":"2023-12-09T12:00:38.622417Z","shell.execute_reply.started":"2023-12-09T12:00:37.802717Z","shell.execute_reply":"2023-12-09T12:00:38.621426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### QLoRA Configuration","metadata":{}},{"cell_type":"code","source":"# looking into the model structure\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T12:00:38.623747Z","iopub.execute_input":"2023-12-09T12:00:38.624619Z","iopub.status.idle":"2023-12-09T12:00:38.632588Z","shell.execute_reply.started":"2023-12-09T12:00:38.624576Z","shell.execute_reply":"2023-12-09T12:00:38.631598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nlora_alpha = 16\nlora_dropout = 0.05 \nlora_r = 8 # rank\n\n# Parameter efficient finetuning for LoRA configuration\n\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    target_modules= [\"q_proj\",\"v_proj\"], # we will only create adopters for q, v metrices of attention module\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T12:00:38.633586Z","iopub.execute_input":"2023-12-09T12:00:38.633862Z","iopub.status.idle":"2023-12-09T12:00:38.667435Z","shell.execute_reply.started":"2023-12-09T12:00:38.633839Z","shell.execute_reply":"2023-12-09T12:00:38.666753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finetuning Process","metadata":{}},{"cell_type":"code","source":"# defining the model fine tuning arguments\n# arguments are self explanatory\n\nimport transformers\n\ntraining_arguments = transformers.TrainingArguments(\n        output_dir=\"llama2_qlora_finetuned\",\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        optim=\"paged_adamw_8bit\",\n        learning_rate=2e-4,\n        lr_scheduler_type=\"linear\",\n        save_strategy=\"epoch\",\n        logging_steps=10,\n        num_train_epochs=3,\n        max_steps=100,\n        fp16=True,\n        push_to_hub=False\n    )","metadata":{"execution":{"iopub.status.busy":"2023-12-09T12:00:38.668506Z","iopub.execute_input":"2023-12-09T12:00:38.668792Z","iopub.status.idle":"2023-12-09T12:00:38.694069Z","shell.execute_reply.started":"2023-12-09T12:00:38.668767Z","shell.execute_reply":"2023-12-09T12:00:38.693182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# creating trainer with the training agruments\n\nfrom trl import SFTTrainer\ntrainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset,\n        peft_config=peft_config, # passing peft config\n        dataset_text_field=\"text\", # mentioned the required column\n        args=training_arguments, # training agruments\n        tokenizer=tokenizer, # tokenizer \n        packing=False,\n        max_seq_length=512\n    )","metadata":{"execution":{"iopub.status.busy":"2023-12-09T12:00:38.695168Z","iopub.execute_input":"2023-12-09T12:00:38.695526Z","iopub.status.idle":"2023-12-09T12:00:53.158488Z","shell.execute_reply.started":"2023-12-09T12:00:38.695492Z","shell.execute_reply":"2023-12-09T12:00:53.157733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# upcasting the layer norms in float 32 for more stable training\n\nfor name, module in trainer.model.named_modules():\n    if \"norm\" in name:\n        module = module.to(torch.float32)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T12:00:53.159756Z","iopub.execute_input":"2023-12-09T12:00:53.160407Z","iopub.status.idle":"2023-12-09T12:00:53.169681Z","shell.execute_reply.started":"2023-12-09T12:00:53.160366Z","shell.execute_reply":"2023-12-09T12:00:53.168798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# starting the finetuning process\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-09T12:00:53.170912Z","iopub.execute_input":"2023-12-09T12:00:53.171240Z","iopub.status.idle":"2023-12-09T13:07:58.654880Z","shell.execute_reply.started":"2023-12-09T12:00:53.171212Z","shell.execute_reply":"2023-12-09T13:07:58.653801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Save the LoRA adopters / you can even push these adopters to hugging face model hub for future inference","metadata":{}},{"cell_type":"code","source":"model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\nmodel_to_save.save_pretrained(\"outputs\")","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:07:58.656285Z","iopub.execute_input":"2023-12-09T13:07:58.656714Z","iopub.status.idle":"2023-12-09T13:07:58.707911Z","shell.execute_reply.started":"2023-12-09T13:07:58.656661Z","shell.execute_reply":"2023-12-09T13:07:58.706966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding back the LoRA adopters to the base Llama-2 model\n\nlora_config = LoraConfig.from_pretrained('outputs')\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:07:58.709137Z","iopub.execute_input":"2023-12-09T13:07:58.709510Z","iopub.status.idle":"2023-12-09T13:07:58.809630Z","shell.execute_reply.started":"2023-12-09T13:07:58.709475Z","shell.execute_reply":"2023-12-09T13:07:58.808712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference using Llama2 + QLoRA adopters","metadata":{}},{"cell_type":"code","source":"# perform inference on the first row of the test dataset\ntext = test_df['text'][0]\nprint(text)","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:07:58.810790Z","iopub.execute_input":"2023-12-09T13:07:58.811052Z","iopub.status.idle":"2023-12-09T13:07:58.817524Z","shell.execute_reply.started":"2023-12-09T13:07:58.811028Z","shell.execute_reply":"2023-12-09T13:07:58.816351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b>Update:</b>\n\nAdded repetition_penalty=1.2 to avoid the repetion of input task as input","metadata":{}},{"cell_type":"code","source":"inputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=100 ,repetition_penalty=1.2)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:07:58.818847Z","iopub.execute_input":"2023-12-09T13:07:58.819307Z","iopub.status.idle":"2023-12-09T13:09:06.380889Z","shell.execute_reply.started":"2023-12-09T13:07:58.819282Z","shell.execute_reply":"2023-12-09T13:09:06.379910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# another example:\ntext = test_df['text'][100]\n\ninputs = tokenizer(text, return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_new_tokens=100,repetition_penalty=1.2)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"execution":{"iopub.status.busy":"2023-12-09T13:09:06.385316Z","iopub.execute_input":"2023-12-09T13:09:06.385622Z","iopub.status.idle":"2023-12-09T13:10:09.191397Z","shell.execute_reply.started":"2023-12-09T13:09:06.385595Z","shell.execute_reply":"2023-12-09T13:10:09.190309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}